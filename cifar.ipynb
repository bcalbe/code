{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import  Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Files already downloaded and verified\nFiles already downloaded and verified\n"
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                    download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n",
    "                                    nn.BatchNorm2d(16),\n",
    "                                    nn.ReLU())\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "                                    nn.BatchNorm2d(32),\n",
    "                                    nn.ReLU())\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2,stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_layer1 = self.layer1(x)\n",
    "        x1 = self.pool1(x_layer1)\n",
    "        x_layer2 = self.layer2(x1)\n",
    "        x2 = self.pool2(x_layer2)\n",
    "        # fully connect\n",
    "        x2 = x2.view(-1, 32*8*8)\n",
    "        x_fc1 = F.relu(self.fc1(x2))\n",
    "        x_fc2 = F.relu(self.fc2(x_fc1))\n",
    "        x_fc3 = self.fc3(x_fc2)\n",
    "        return x_fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1,  2000] average loss: 1.807\n[1,  4000] average loss: 1.550\n[1,  6000] average loss: 1.424\n"
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# SGD with momentum\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for iter in range(1):\n",
    "    for epoch in range(5):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            # warp them in Variable\n",
    "            # inputs, labels = Variable(inputs), Variable(labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            # loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print('[%d, %5d] average loss: %.3f' %\n",
    "                    (iter*5+epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images,labels = data\n",
    "    outputs = net(images)\n",
    "    _,predicted = torch.max(outputs,1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  }
 ]
}